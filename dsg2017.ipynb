{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-0.6a2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 649kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): numpy in /opt/conda/lib/python3.5/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /opt/conda/lib/python3.5/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /opt/conda/lib/python3.5/site-packages (from xgboost)\n",
      "Building wheels for collected packages: xgboost\n",
      "  Running setup.py bdist_wheel for xgboost ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5e/c1/d6/522af54e5cc001fad4dd855117f8bf61b11d56443e06672e26\n",
      "Successfully built xgboost\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.6a2\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting h2o\n",
      "  Downloading h2o-3.10.4.3-py2.py3-none-any.whl (60.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 60.3MB 25kB/s \n",
      "\u001b[?25hCollecting tabulate (from h2o)\n",
      "  Downloading tabulate-0.7.7-py2.py3-none-any.whl\n",
      "Collecting future (from h2o)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.7MB/s \n",
      "\u001b[?25hCollecting colorama (from h2o)\n",
      "  Downloading colorama-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /opt/conda/lib/python3.5/site-packages (from h2o)\n",
      "Building wheels for collected packages: future\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "Successfully built future\n",
      "Installing collected packages: tabulate, future, colorama, h2o\n",
      "Successfully installed colorama-0.3.7 future-0.16.0 h2o-3.10.4.3 tabulate-0.7.7\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.13.0-cp35-cp35m-manylinux1_x86_64.whl (33.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 33.8MB 46kB/s \n",
      "\u001b[?25hCollecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "  Downloading PyWavelets-0.5.2-cp35-cp35m-manylinux1_x86_64.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.7MB 271kB/s \n",
      "\u001b[?25hCollecting scipy>=0.17.0 (from scikit-image)\n",
      "  Downloading scipy-0.19.0-cp35-cp35m-manylinux1_x86_64.whl (47.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 47.9MB 33kB/s \n",
      "\u001b[?25hCollecting networkx>=1.8 (from scikit-image)\n",
      "  Downloading networkx-1.11-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): six>=1.7.3 in /opt/conda/lib/python3.5/site-packages (from scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pillow>=2.1.0 in /opt/conda/lib/python3.5/site-packages (from scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): matplotlib>=1.3.1 in /opt/conda/lib/python3.5/site-packages (from scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.9.1 in /opt/conda/lib/python3.5/site-packages (from PyWavelets>=0.4.0->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): decorator>=3.4.0 in /opt/conda/lib/python3.5/site-packages (from networkx>=1.8->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): olefile in /opt/conda/lib/python3.5/site-packages (from pillow>=2.1.0->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /opt/conda/lib/python3.5/site-packages (from matplotlib>=1.3.1->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz in /opt/conda/lib/python3.5/site-packages (from matplotlib>=1.3.1->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyparsing>=1.5.6 in /opt/conda/lib/python3.5/site-packages (from matplotlib>=1.3.1->scikit-image)\n",
      "Requirement already satisfied (use --upgrade to upgrade): nose>=0.11.1 in /opt/conda/lib/python3.5/site-packages (from matplotlib>=1.3.1->scikit-image)\n",
      "Installing collected packages: PyWavelets, scipy, networkx, scikit-image\n",
      "  Found existing installation: scipy 0.16.1\n",
      "\u001b[33m    DEPRECATION: Uninstalling a distutils installed project (scipy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling scipy-0.16.1:\n",
      "      Successfully uninstalled scipy-0.16.1\n",
      "Successfully installed PyWavelets-0.5.2 networkx-1.11 scikit-image-0.13.0 scipy-0.19.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install h2o\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncode(df, colName):\n",
    "    return pd.get_dummies(df, columns=[colName], sparse=True)\n",
    "def oneHotEncodeAll(df):\n",
    "    return pd.get_dummies(df, sparse=True)\n",
    "\n",
    "def readData(path, nrows = 1000):\n",
    "#     data = np.genfromtxt(path, delimiter=',')\n",
    "    input_data = pd.read_csv(path, nrows=nrows)\n",
    "    input_data[\"genre_id\"] = input_data[\"genre_id\"].astype(\"category\")\n",
    "    input_data[\"media_id\"] = input_data[\"media_id\"].astype(\"category\")\n",
    "    input_data[\"album_id\"] = input_data[\"album_id\"].astype(\"category\")\n",
    "    input_data[\"context_type\"] = input_data[\"context_type\"].astype(\"category\")\n",
    "    input_data[\"platform_name\"] = input_data[\"platform_name\"].astype(\"category\")\n",
    "    input_data[\"platform_family\"] = input_data[\"platform_family\"].astype(\"category\")\n",
    "    input_data[\"listen_type\"] = input_data[\"listen_type\"].astype(\"category\")\n",
    "    input_data[\"user_gender\"] = input_data[\"user_gender\"].astype(\"category\")\n",
    "    input_data[\"user_id\"] = input_data[\"user_id\"].astype(\"category\")\n",
    "    input_data[\"artist_id\"] = input_data[\"artist_id\"].astype(\"category\")\n",
    "    return input_data.drop([\"ts_listen\", \"release_date\"], axis = 1)\n",
    "#print(train_data.irow(0))\n",
    "\n",
    "def extractFeatureLabel(df):\n",
    "    return df.drop([\"is_listened\"], axis=1), df[\"is_listened\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7558834, 13)\n"
     ]
    }
   ],
   "source": [
    "train_data = readData(\"./train.csv\", nrows=None)\n",
    "# train_data = train_data.sample(frac=0.3)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
       "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(max_depth=20, learning_rate=0.1, n_estimators=300)\n",
    "model.fit(train_data.drop([\"is_listened\"], axis=1).values, train_data[\"is_listened\"].values, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = readData(\"./test.csv\", nrows=None)\n",
    "n_test = test_data.shape[0]\n",
    "test_data = test_data.drop([\"sample_id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.predict(data=test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_file = open(\"Output_xgboost.csv\", \"w\")\n",
    "text_file.write(\"sample_id,is_listened\" + \"\\n\") #sample_id,is_listened\n",
    "for i in range(len(result)):\n",
    "    text_file.write(str(i) + \",\" + str(result[i]) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a92463473296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dsg17_xgbclassifier.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(model, open(\"dsg17_xgbclassifier.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = oneHotEncodeAll(train_data)\n",
    "#train_data.to_csv(\"train_pre_1.csv\",index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read and convert done\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain = extractFeatureLabel(train_data)\n",
    "print(\"Read and convert done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTrain = pd.get_dummies(train_data, \n",
    "                            columns=[\"genre_id\", \n",
    "                                     \"context_type\", \"platform_name\", \"platform_family\"], sparse = True)\n",
    "#\"genre_id\", \"media_id\", \"album_id\", \n",
    " #                                    \"context_type\", \"platform_name\", \"platform_family\", \"artist_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: sklearn random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.ensemble as ens\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def readData(path, nrows = 1000):\n",
    "#     data = np.genfromtxt(path, delimiter=',')\n",
    "    input_data = pd.read_csv(path, nrows=nrows)\n",
    "    input_data[\"genre_id\"] = input_data[\"genre_id\"].astype(\"category\")\n",
    "    input_data[\"media_id\"] = input_data[\"media_id\"].astype(\"category\")\n",
    "    input_data[\"album_id\"] = input_data[\"album_id\"].astype(\"category\")\n",
    "    input_data[\"context_type\"] = input_data[\"context_type\"].astype(\"category\")\n",
    "    input_data[\"platform_name\"] = input_data[\"platform_name\"].astype(\"category\")\n",
    "    input_data[\"platform_family\"] = input_data[\"platform_family\"].astype(\"category\")\n",
    "    input_data[\"listen_type\"] = input_data[\"listen_type\"].astype(\"category\")\n",
    "    input_data[\"user_gender\"] = input_data[\"user_gender\"].astype(\"category\")\n",
    "    input_data[\"user_id\"] = input_data[\"user_id\"].astype(\"category\")\n",
    "    input_data[\"artist_id\"] = input_data[\"artist_id\"].astype(\"category\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7558834, 15)\n"
     ]
    }
   ],
   "source": [
    "train_data = readData(\"./train.csv\", nrows=None)\n",
    "# train_data = train_data.sample(frac=0.3)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=11, n_jobs=2,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ens.RandomForestClassifier(n_estimators=11, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                        min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                        max_leaf_nodes=None, bootstrap=True, oob_score=False, \n",
    "                                        n_jobs=2, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "classifier.fit(train_data.drop([\"is_listened\"], axis=1).values, train_data[\"is_listened\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = readData(\"./test.csv\", nrows=None)\n",
    "n_test = test_data.shape[0]\n",
    "test_data = test_data.drop([\"sample_id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11718\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_data.values)\n",
    "print(sum(result))\n",
    "text_file = open(\"Output1.csv\", \"w\")\n",
    "text_file.write(\"sample_id,is_listened\" + \"\\n\") #sample_id,is_listened\n",
    "for i in range(len(result)):\n",
    "    text_file.write(str(i) + \",\" + str(result[i]) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: One hot coding preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.preprocessing as skpre\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def readData(path, nrows = 1000):\n",
    "#     data = np.genfromtxt(path, delimiter=',')\n",
    "    input_data = pd.read_csv(path, nrows=nrows)\n",
    "#     input_data[\"genre_id\"] = input_data[\"genre_id\"].astype(\"category\")\n",
    "#     input_data[\"media_id\"] = input_data[\"media_id\"].astype(\"category\")\n",
    "#     input_data[\"album_id\"] = input_data[\"album_id\"].astype(\"category\")\n",
    "#     input_data[\"context_type\"] = input_data[\"context_type\"].astype(\"category\")\n",
    "#     input_data[\"platform_name\"] = input_data[\"platform_name\"].astype(\"category\")\n",
    "#     input_data[\"platform_family\"] = input_data[\"platform_family\"].astype(\"category\")\n",
    "#     input_data[\"listen_type\"] = input_data[\"listen_type\"].astype(\"category\")\n",
    "#     input_data[\"user_gender\"] = input_data[\"user_gender\"].astype(\"category\")\n",
    "#     input_data[\"user_id\"] = input_data[\"user_id\"].astype(\"category\")\n",
    "#     input_data[\"artist_id\"] = input_data[\"artist_id\"].astype(\"category\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = readData(\"./train.csv\", nrows=None)\n",
    "# train_data = train_data.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_feature, train_label = train_data.drop([\"is_listened\"], axis=1).values, train_data[\"is_listened\"].values\n",
    "print(type(train_feature))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feature, train_label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7558834, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# genre_id, ts_listen, media_id, album_id, context_type, release_date, platform_name, platform_family, media_duration, listen_type, user_gender, user_id, artist_id, user_age\n",
    "# 0         1          2         3         4              5            6              7                8                9           10           11       12         13\n",
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 2, 3, 4, 6, 7], dtype=<class 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = skpre.OneHotEncoder(n_values='auto', categorical_features=[0, 2, 3, 4, 6, 7],\n",
    "                                 sparse=True, handle_unknown='error')\n",
    "encoder.fit(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_one_hot = encoder.transform(X_train)\n",
    "X_test_one_hot = encoder.transform(X_test)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "classifier = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=20)\n",
    "classifier.fit(X_train_one_hot, y_train, verbose = True)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698569123995\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(X_test_one_hot)\n",
    "print(sum(result == y_test) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13013\n"
     ]
    }
   ],
   "source": [
    "test_data = readData(\"./test.csv\", nrows=None)\n",
    "n_test = test_data.shape[0]\n",
    "test_data = test_data.drop([\"sample_id\"], axis = 1)\n",
    "result = classifier.predict(encoder.transform(test_data.values))\n",
    "print(sum(result))\n",
    "text_file = open(\"Output1.csv\", \"w\")\n",
    "text_file.write(\"sample_id,is_listened\" + \"\\n\") #sample_id,is_listened\n",
    "for i in range(len(result)):\n",
    "    text_file.write(str(i) + \",\" + str(result[i]) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes: failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683747618681\n",
      "19918\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "classifier = nb.BernoulliNB(binarize=1.0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "result = classifier.predict(X_test)\n",
    "print(sum(result == y_test) / len(result))\n",
    "\n",
    "test_data = readData(\"./test.csv\", nrows=None)\n",
    "n_test = test_data.shape[0]\n",
    "test_data = test_data.drop([\"sample_id\"], axis = 1)\n",
    "result = classifier.predict(test_data.values)\n",
    "print(sum(result))\n",
    "text_file = open(\"Output_Naive_bayes.csv\", \"w\")\n",
    "text_file.write(\"sample_id,is_listened\" + \"\\n\") #sample_id,is_listened\n",
    "for i in range(len(result)):\n",
    "    text_file.write(str(i) + \",\" + str(result[i]) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Svd:\n",
    "\n",
    "svd user x artist, user x genre, user x album, replace user, artist, genre with vector from svd, train neural network, SVM\n",
    "remove release date and ts_listen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.preprocessing as skpre\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def readData(path, nrows = 1000):\n",
    "#     data = np.genfromtxt(path, delimiter=',')\n",
    "    input_data = pd.read_csv(path, nrows=nrows)\n",
    "#     input_data[\"genre_id\"] = input_data[\"genre_id\"].astype(\"category\")\n",
    "#     input_data[\"media_id\"] = input_data[\"media_id\"].astype(\"category\")\n",
    "#     input_data[\"album_id\"] = input_data[\"album_id\"].astype(\"category\")\n",
    "#     input_data[\"context_type\"] = input_data[\"context_type\"].astype(\"category\")\n",
    "#     input_data[\"platform_name\"] = input_data[\"platform_name\"].astype(\"category\")\n",
    "#     input_data[\"platform_family\"] = input_data[\"platform_family\"].astype(\"category\")\n",
    "#     input_data[\"listen_type\"] = input_data[\"listen_type\"].astype(\"category\")\n",
    "#     input_data[\"user_gender\"] = input_data[\"user_gender\"].astype(\"category\")\n",
    "#     input_data[\"user_id\"] = input_data[\"user_id\"].astype(\"category\")\n",
    "#     input_data[\"artist_id\"] = input_data[\"artist_id\"].astype(\"category\")\n",
    "    return input_data.drop([\"ts_listen\", \"release_date\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = readData(\"./train.csv\", nrows=None)\n",
    "# train_data = train_data.sample(frac=0.3)\n",
    "train_feature, train_label = train_data.drop([\"is_listened\"], axis=1).values, train_data[\"is_listened\"].values\n",
    "print(type(train_feature))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feature, train_label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to spark dataframe\n",
    "train_data_spark = sqlContext.createDataFrame(train_data)\n",
    "all_data = train_data_spark.rdd.map(lambda x: Rating(x[10], x[1], (x[-1] * 2 - 1) * 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ALS.trainImplicit(all_data, rank = 10, iterations=6, lambda_=1.0, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = readData(\"./test.csv\", nrows=None)\n",
    "n_test = test_data.shape[0]\n",
    "test_data = test_data.drop([\"sample_id\"], axis = 1)\n",
    "test_data = sqlContext.createDataFrame(test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(sum(result))\n",
    "# text_file = open(\"Output1.csv\", \"w\")\n",
    "# text_file.write(\"sample_id,is_listened\" + \"\\n\") #sample_id,is_listened\n",
    "# for i in range(len(result)):\n",
    "#     text_file.write(str(i) + \",\" + str(result[i]) + \"\\n\")\n",
    "# text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
